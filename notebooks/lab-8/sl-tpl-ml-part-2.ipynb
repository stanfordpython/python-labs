{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Machine Learning\n",
    "\n",
    "## Overview\n",
    "In the second part of our \\*sniff\\* *FINAL* \\*sniff\\* lab, we're going to explore some of the machine learning models that we discussed in Monday's lecture. We'll be playing around a lot with scikit-learn and applying it to real-world dataset. There won't be many new algorithms in this lab that we didn't see in lecture; instead, this will give you the chance to play around with models, feature selection, and hyperparameters to develop a better intuition for how machine learning models behave on interesting data.\n",
    "\n",
    "If this setup looks similar to what we did in Monday's lecture, that's because it is pretty similar. Monday's lecture covered a lot of ground, and especially if you're less familiar with machine learning in other contexts, the hope is this lab is a friendly environment in which to get a bit more comfortable with how ML works in the real world.\n",
    "\n",
    "Let's dive in!\n",
    "\n",
    "## The Dataset\n",
    "Predicting health insurance charges is an important problem in the United States. In this task, we will predict the health insurance charges to patients as a function of various health data features about the patient.\n",
    "\n",
    "[The dataset](https://www.kaggle.com/mirichoi0218/insurance) consists of 1338 examples of health insurance charges. The features of each example are (in order) as follows:\n",
    "\n",
    "- age: age of primary beneficiary\n",
    "- sex: insurance contractor gender, female, male (1 female, 0 male)\n",
    "- bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,\n",
    "- objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n",
    "- children: Number of children covered by health insurance / Number of dependents\n",
    "- smoker: Smoking\n",
    "- region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest. (1 northeast, 2 northwest, 3 southeast, 4 southwest)\n",
    "\n",
    "The final column in the dataset is the cost billed by the health insurance, and is the variable we are trying to predict:\n",
    "- charges: Individual medical costs billed by health insurance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "Let's first run the following cell to import the required packages and load our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "def load_dataset_regression(filename):\n",
    "    \"\"\"\n",
    "    Loads the dataset from the file. Shuffles it, then\n",
    "    adds it to NumPy matrices.\n",
    "\n",
    "    Input:\n",
    "    filename - the name of a .csv file to load.\n",
    "\n",
    "    Output:\n",
    "    X, Y - NumPy arrays extracted from the .csv file. X is\n",
    "           the data, Y is the labels.\n",
    "    \"\"\"\n",
    "    # The skiprows=1 argument skips the first row, where the data labels are\n",
    "    with open(filename, \"r\") as f:\n",
    "        dataset = np.loadtxt(f, delimiter=\",\", skiprows=1)\n",
    "\n",
    "    # Shuffle the dataset before splitting into train, validation, and test sets - this is\n",
    "    # important to ensure an even distribution of datapoints in the train, validation,\n",
    "    # and test sets.\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    # X is all columns except the \"csMPa\" column\n",
    "    X = dataset[:,:-1]\n",
    "    # Y is the \"csMPa\" column\n",
    "    Y = dataset[:,-1]\n",
    "    return X, Y\n",
    "\n",
    "X, Y = load_dataset_regression(\"insurance.csv\")\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "Let's get a better intuition for our data by running the following cell, which visualizes our output variable as a function of each of the input variables.\n",
    "\n",
    "We'll be diving more into plotting when we chat next week about MatPlotLib, but for now we can take this function as a given and use it to visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def view_data(X, Y):\n",
    "    \"\"\"\n",
    "    Produces plots of each variable in X with respect to\n",
    "    the prediction variable of interest, Y.\n",
    "\n",
    "    Input:\n",
    "    X, Y - NumPy arrays representing features and labels,\n",
    "           respectively.\n",
    "    \n",
    "    Output:\n",
    "    None - but visualizes graphs with plt.show()\n",
    "    \"\"\"\n",
    "    feature_labels = [\"Age\", \"Sex\", \"BMI\", \"Children\", \"Smoker\", \"Region\"]\n",
    "    for col in range(X.shape[1]):\n",
    "        # Plot each column with respect to Y to look at variable relationship.\n",
    "        plt.clf()\n",
    "        plt.scatter(X[:, col], Y)\n",
    "        plt.xlabel(feature_labels[col])\n",
    "        plt.ylabel(\"Insurance Charges\")\n",
    "        plt.show()\n",
    "        \n",
    "view_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenting Our Data\n",
    "Start by writing the following function to divide the data up into training, validation, and test sets. Use a split of 80% training set, 10% validation set, 10% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y):\n",
    "    \"\"\"\n",
    "    Given data and labels, divides the data into training,\n",
    "    validation, and test sets, with an 80%-10%-10% split.\n",
    "    \n",
    "    Input:\n",
    "    X, Y - NumPy arrays consisting of features and labels,\n",
    "           respectively.\n",
    "\n",
    "    Output:\n",
    "    X_train, Y_train, X_valid, Y_valid, X_test, Y_test - NumPy arrays representing the\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Least Squares\n",
    "\n",
    "To get started, implement a least squares model in the following function. Train the model on your training set, print out the loss, then print out your predictions on the validation set. How well does a linear model perform on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_linear_regression(X_train, Y_train, X_valid, Y_valid):\n",
    "    \"\"\"\n",
    "    Run least squares regression on the training set. Then print out predictions\n",
    "    on the validation set.\n",
    "    \n",
    "    Input:\n",
    "    X_train, Y_train, X_valid, Y_valid - NumPy arrays representing the\n",
    "                                         training and validation sets.\n",
    "\n",
    "    Output:\n",
    "    [ Y_hat_valid Y_valid] - NumPy array, the left column of which consists\n",
    "                             of predictions on X_valid, the right column of which\n",
    "                             consists of Y_valid. \n",
    "                             \n",
    "    Prints out the squared loss.\n",
    "    \n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \n",
    "least_squares_linear_regression(X_train, Y_train, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Feature Selection (Optional)\n",
    "\n",
    "Look back at the graphs for this dataset - this dataset is a little bit _messy_. Well, it's not awful, actually, but there's pretty high variance across variables, which makes any sort of linear model a bit of a tricky fit at best.\n",
    "\n",
    "If you're feeling up to it, choose a feature of the dataset that you feel is better represented by a nonlinear function, and add a new nonlinear column to the dataset. Train least squares linear regression on your newly-augmented dataset, print out the loss, and compare it with the loss from when you implemented least squares with no nonlinearities. How low are you able to get the loss?\n",
    "\n",
    "(Hint: there really aren't any features that are just screaming at you to pick a certain nonlinear function, like those we saw in the lecture examples. Don't worry! Here are a couple suggestions: consider adding a feature which is the log of the age column, as there's significant density in the bottom left of the graph. Alternatively, choose a feature (any feature!) and find the point at which increasing the degree of a polynomial fit begins to decrease the accuracy. The possibilities here are endless!!)\n",
    "\n",
    "Note: consider using `np.concatenate` to add a new column onto your existing feature matrices! ðŸ™ƒ\n",
    "\n",
    "\n",
    "### Example\n",
    "This was one of the trickiest parts of Monday's lecture, so let's walk through a quick example to cement things if you're feeling a little less sure on this part!\n",
    "\n",
    "Let's walk quickly through why nonlinearities are such neat features in linear models. Assume for a second that we've got a column, $x$, of the following form:\n",
    "$\n",
    "x = \\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\vdots\\\\\n",
    "x_n\\\\\n",
    "\\end{bmatrix}$, and a labels vector, $y$, of the following form: $\n",
    "y = \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\vdots\\\\\n",
    "y_n\\\\\n",
    "\\end{bmatrix}$. \n",
    "\n",
    "\n",
    "Assume for a moment that $y_i \\approx x_i^2$, but this is only a rough approximation. Least squares is a linear model - as we've seen in lecture, it entails fitting a line to the data, such that we predict $y_i = \\theta x_i$. How on earth can we possibly hope to fit this data effectively?\n",
    "\n",
    "\n",
    "It turns out that, if we have an understanding of the function mapping $x$ to $y$, we can add the appropriate features to our feature vector in order to dramatically improve performance. We then construct the following feature vector, $x'$, from our old one, $x$: $x' = \\begin{bmatrix}\n",
    "x_1 & x_1^2\\\\\n",
    "x_2 & x_2^2\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "x_n & x_n^2\n",
    "\\end{bmatrix}$. \n",
    "\n",
    "Assuming we've got our same $y$, a good model will choose thetas $\\theta_1, \\theta_2$ such that $\\theta_1 x_i + \\theta_2 x_i^2 \\approx y_i$. Since we know already that $y_i \\approx x_i^2$, we can see that a pretty nice approximation is obtained with $\\theta_1 = 0, \\theta_2 = 1$.\n",
    "\n",
    "Let's take a quick step back and evaluate what we've just done. We've taken highly, _highly_ nonlinear data, and approximated it with a linear model. _We used a line to model something that isn't a line._ Isn't that incredible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_least_squares(X_train, Y_train, X_valid, Y_valid):\n",
    "    \"\"\"\n",
    "    Perform feature selection to construct a new X_train, X_valid using a\n",
    "    function and feature of your choice. Then run least squares on the new\n",
    "    X_train, Y_train, and make predictions on the new X_valid and the Y_valid.\n",
    "    \n",
    "    Print out the loss from running least squares without feature selection\n",
    "    (so running it on the old X_train, X_valid), and the loss from running\n",
    "    least squares with feature selection. Can you select functions which \n",
    "    reduce the accuracy below that of a linear model without feature selection?\n",
    "    \n",
    "    (If so - super cool!! If not - what might this tell you about the dataset?)\n",
    "    \n",
    "    Input:\n",
    "    X_train, Y_train, X_valid, Y_valid - NumPy arrays representing the\n",
    "                                         training and validation sets.\n",
    "\n",
    "    Output:\n",
    "    None - prints out validation loss with and without feature selection.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks - The Cutting Edge!!\n",
    "\n",
    "We're finally here... neural networks! Deep learning! The new electricity!\n",
    "\n",
    "Using scikit-learn's MLPRegressor neural network, try training a neural regressor on the training set. Print out your predictions on the validation set and the validation set labels. How close are your predictions? Play around with the hyperparameters, such as the numbers of nodes, the step size for logistic regression, and the loss, until you're satisfied with how close your predictions are to the real dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def NN_regressor(X_train, Y_train, X_valid, Y_valid):\n",
    "    \"\"\"\n",
    "    Trains a neural network to perform regression on the dataset. Makes predictions\n",
    "    on the validation set, and prints out the validation loss from a least squares\n",
    "    regression versus that of the neural network regression.\n",
    "\n",
    "    Input:\n",
    "    X_train, Y_train, X_valid, Y_valid - NumPy arrays representing the\n",
    "                                         training and validation sets.\n",
    "    \n",
    "    Output:\n",
    "    None - prints out validation loss of NN and least squares regression.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def NN_regressor_test(X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    When you feel like you're doing a pretty good job on your validation set, try\n",
    "    training your neural network on the training set, then running your neural \n",
    "    network on the test set! Print out yuor predictions and the true test set values.\n",
    "    How close are your predictions?\n",
    "    \n",
    "    Remember that we use the test set once - and only once - to remove experimenter\n",
    "    bias in the process. In this way, experimentors can't tune their hyperparameters\n",
    "    to ensure a good fit on the test set, when in reality, their model may fail to\n",
    "    generalize to new example cases that come in.\n",
    "    \n",
    "    Input:\n",
    "    X_train, Y_train, X_test, Y_test - NumPy arrays representing the\n",
    "                                       training and test sets.\n",
    "    \n",
    "    Output:\n",
    "    None - prints out test labels and predictions.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \n",
    "NN_regressor(X_train, Y_train, X_valid, Y_valid)\n",
    "# Uncomment the next line when you're ready to run the test set!\n",
    "# NN_regressor_test(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
